import requests
from bs4 import BeautifulSoup
from typing import List, Dict # ğŸ¯ Flaskì™€ ë°ì´í„° í˜•ì‹ì„ ë§ì¶”ê¸° ìœ„í•´ import

# --- (í—¬í¼ í•¨ìˆ˜: scrape_data_from_soup) ---
# ì´ í•¨ìˆ˜ëŠ” ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
def scrape_data_from_soup(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """í•´ë‹¹ í˜ì´ì§€ì˜ ì§ì—… ëª©ë¡ì„ íŒŒì‹±."""
    
    page_jobs = [] 
    jobs = soup.find_all("tr", class_="table_row")
    
    for job in jobs:
        title_tag = job.find("h2", class_="my-primary")
        title = title_tag.get_text(strip=True) if title_tag else ""
    
        if not title:
            continue
    
        company_tag = job.find("h3")
        company = company_tag.get_text(strip=True) if company_tag else ""
    
        # ìœ„ì¹˜
        location_tds = job.find_all("td", class_="job-location-mobile")
        locations = []
        if len(location_tds) > 1:
            location_td = location_tds[1]
            for a_tag in location_td.find_all("a"):
                locations.append(a_tag.get_text(strip=True))
        location = ", ".join(locations)
    
        # ìŠ¤íƒ
        stack_spans = job.find_all("span", class_="my-badge my-badge-secondary")
        stack_list = [span.get_text(strip=True) for span in stack_spans]
        stack = ", ".join(stack_list)

        link_tag = title_tag.find("a", href=True) if title_tag else None
        link = "https://web3.career" + link_tag['href'] if link_tag and link_tag['href'].startswith("/") else "N/A"
    
        job_data = {
            "position": title,
            "company": company,
            "condition": location,
            "link": link,
        }
        page_jobs.append(job_data)
        
    return page_jobs

def extract_webs_jobs(keyword: str) -> List[Dict[str, str]]:
    """
    'keyword'ë¥¼ ë°›ì•„ í•´ë‹¹ ì§ë¬´ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    """
    
    all_jobs = []
    current_page = 1 # 1í˜ì´ì§€ë¶€í„° ì‹œì‘
    
    base_url = f"https://web3.career/{keyword}-jobs"

    while True:
        url = f"{base_url}?page={current_page}"
        # print(f"Scraping: {url}") # (ì„œë²„ ë¡œê·¸)
        
        try:
            response = requests.get(url)
            response.raise_for_status() 
        except requests.RequestException as e:
            print(f"URL ìš”ì²­ ì‹¤íŒ¨: {url} (ì—ëŸ¬: {e}). ìŠ¤í¬ë˜í•‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
    
        soup = BeautifulSoup(response.content, "html.parser")
        
        jobs_on_this_page = scrape_data_from_soup(soup)
        
        if not jobs_on_this_page:
            print(f"í˜ì´ì§€ {current_page}ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
            
        all_jobs.extend(jobs_on_this_page)
        # print(f"--- {current_page} í˜ì´ì§€ ì™„ë£Œ, {len(jobs_on_this_page)}ê°œ ìˆ˜ì§‘ (ì´ {len(all_jobs)}ê°œ) ---")
    
        next_disabled_button = soup.select_one("li.page-item.next.disabled")
        
        if next_disabled_button:
            # print("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break 
        
        current_page += 1

    print(f"\n--- [WEBS] {keyword} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ---")
    print(f"ì´ {len(all_jobs)}ê°œì˜ ì§ì—… ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    return all_jobs


if __name__ == "__main__":
    # 'python webs.py'ë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ë©´ ì´ ë¶€ë¶„ì´ ë™ì‘í•©ë‹ˆë‹¤.
    jobs = extract_webs_jobs("python")
    print("\n--- [webs.py] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ) ---")
    print(jobs[:3])